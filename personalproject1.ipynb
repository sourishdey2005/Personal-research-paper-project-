{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPde7OJc+2vXwXyQrIv30kt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourishdey2005/Personal-research-paper-project-/blob/main/personalproject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRt8AfFqLnhG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b496e9db"
      },
      "source": [
        "# Task\n",
        "Develop a self-supervised learning model for decoding microbial communication from synthetic metagenomic data to enhance bio-fertilizer design, aiming for over 85% accuracy in communication decoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e20ad7"
      },
      "source": [
        "## Setup Environment and Define Problem\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Python libraries and define the problem by outlining the simulated characteristics of 'metagenomic data' and 'microbial communication networks' for the synthetic dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb9cf94"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step of the subtask is to install the necessary Python libraries. I will use `pip` to install `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, and `torch`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06d65fbb",
        "outputId": "52e95ca5-1459-4102-c38a-73fa0fd2cf26"
      },
      "source": [
        "pip install pandas numpy scikit-learn matplotlib seaborn torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "667018ee"
      },
      "source": [
        "### Defining Synthetic Metagenomic Data and Microbial Communication Networks\n",
        "\n",
        "To simulate a realistic environment for self-supervised learning, we will define the following characteristics for our synthetic dataset:\n",
        "\n",
        "#### 1. Characteristics of Synthetic Metagenomic Data:\n",
        "\n",
        "*   **Microbial Features (Individual Microbes):**\n",
        "    *   **Functional Genes/Metabolic Capabilities:** Represented as binary vectors or numerical scores indicating the presence/absence or abundance of specific metabolic pathways or functional genes (e.g., genes for nitrogen fixation, phosphate solubilization, antibiotic production, degradation of specific compounds). For instance, a microbe might have `[1, 0, 1, 0]` for nitrogen fixation, antibiotic resistance, quorum sensing, and cellulose degradation, respectively.\n",
        "    *   **Taxonomic Identity:** Represented as categorical labels or one-hot encoded vectors, indicating the species or genus of the microbe. This could influence its baseline metabolic profile and interaction patterns.\n",
        "    *   **Growth Rate Potential:** A numerical value indicating the inherent growth potential under ideal conditions, which can be modulated by interactions.\n",
        "    *   **Resource Preferences:** Numerical scores for uptake rates of various essential nutrients (e.g., carbon, nitrogen, phosphorus sources). For example, `[0.8, 0.2, 0.5]` for glucose, ammonia, and phosphate uptake efficiency.\n",
        "\n",
        "*   **Encoding:** Features will primarily be encoded as numerical vectors (binary or floating-point) or one-hot encoded categorical variables, making them suitable for neural network input.\n",
        "\n",
        "#### 2. Characteristics of Microbial Communication Networks:\n",
        "\n",
        "*   **Communication Signals/Interaction Types:**\n",
        "    *   **Resource Exchange (Commensalism/Mutualism):** Represented by the transfer of metabolic byproducts or essential nutrients from one microbe to another. This can be modeled as a directed interaction where one microbe produces a resource that another consumes.\n",
        "    *   **Quorum Sensing Molecules:** Specific signaling molecules produced and detected by microbes, influencing gene expression and collective behaviors. The presence and concentration of these molecules can trigger changes in microbial state or growth.\n",
        "    *   **Antagonistic Interactions (Competition/Inhibition):** Modeled as the production of inhibitory compounds (e.g., antibiotics, bacteriocins) by one microbe that negatively impacts the growth or survival of another.\n",
        "    *   **Synergistic Interactions:** Cases where two or more microbes collectively perform a function that neither could do alone, potentially through sequential resource conversion or co-factor production.\n",
        "\n",
        "*   **Strength/Nature of Interactions:**\n",
        "    *   **Interaction Strength:** Numerical values representing the magnitude of the effect (e.g., amount of resource exchanged, inhibition strength). This can be a weight on the edge of a graph.\n",
        "    *   **Interaction Type:** Categorical labels (e.g., 'resource_transfer', 'quorum_sensing', 'inhibition') associated with each interaction, allowing for diverse network structures.\n",
        "    *   **Dynamic Nature:** Interactions may not be constant and could depend on environmental factors or the current state (e.g., population density) of the interacting microbes.\n",
        "\n",
        "#### 3. Support for Self-Supervised Learning Task:\n",
        "\n",
        "The model will learn to decode communication patterns by reconstructing masked or perturbed parts of the metagenomic data or interaction network. For example:\n",
        "\n",
        "*   **Masked Feature Prediction:** Predict missing microbial features (e.g., metabolic capabilities) based on observed features and known interactions.\n",
        "*   **Interaction Prediction:** Predict the existence or type of interaction between two microbes given their features.\n",
        "*   **Temporal Prediction:** Predict future states of the microbial community or communication patterns given past observations.\n",
        "*   **Graph Representation Learning:** The model will learn embeddings for microbes and interactions, where similarity in the embedding space reflects similar functional roles or communication partners. The self-supervised task will involve using context (e.g., neighboring microbes in the network) to predict attributes of a central microbe, or predicting the existence of links between microbes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6c7f1e"
      },
      "source": [
        "## Generate Large Synthetic Metagenomic Dataset\n",
        "\n",
        "### Subtask:\n",
        "Develop a function to generate a synthetic dataset of over 5 million entries. This dataset should simulate 'metagenomic' features and implicit 'communication' signals, embedding patterns suitable for self-supervised learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "170137f3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires generating a synthetic dataset with over 5 million entries, simulating metagenomic features and communication signals for self-supervised learning. The first step is to import the necessary libraries and define a function that will create this dataset, including the simulation of microbial features and implicit communication patterns as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b535fa",
        "outputId": "bc332660-898f-40ae-9168-54bc82409052"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_synthetic_data(num_samples):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset simulating metagenomic features and microbial communication signals.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): The number of entries to generate in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the synthetic metagenomic data.\n",
        "    \"\"\"\n",
        "    # 1. Define simulation parameters\n",
        "    num_genes = 15  # Number of functional genes/metabolic capabilities\n",
        "    num_species = 8  # Number of different microbial species/groups\n",
        "    num_resources = 4  # Number of different resource preferences\n",
        "\n",
        "    # Initialize lists to store generated data\n",
        "    data = {\n",
        "        'sample_id': np.arange(num_samples)\n",
        "    }\n",
        "\n",
        "    # 2. Simulate microbial features\n",
        "    # 'Functional Genes/Metabolic Capabilities': binary vectors\n",
        "    for i in range(num_genes):\n",
        "        data[f'gene_{i+1}'] = np.random.randint(0, 2, num_samples)\n",
        "\n",
        "    # 'Taxonomic Identity': categorical (represented numerically for simplicity)\n",
        "    data['taxonomic_id'] = np.random.randint(0, num_species, num_samples)\n",
        "\n",
        "    # 'Growth Rate Potential': numerical\n",
        "    data['growth_rate_potential'] = np.random.rand(num_samples) * 10 # Scale from 0-10\n",
        "\n",
        "    # 'Resource Preferences': numerical values for each resource\n",
        "    for i in range(num_resources):\n",
        "        data[f'resource_pref_{i+1}'] = np.random.rand(num_samples)\n",
        "\n",
        "    # 3. Simulate implicit 'communication' signals or interaction patterns\n",
        "    # This part embeds patterns for self-supervised learning.\n",
        "    # Example: Communication strength based on species and resource preferences\n",
        "    # If species X prefers resource Y, and gene Z is present, communication might be higher.\n",
        "\n",
        "    # Define a base communication signal\n",
        "    data['communication_signal'] = np.random.rand(num_samples) * 5 # Base signal 0-5\n",
        "\n",
        "    # Add patterns based on species and resource preferences\n",
        "    # For simplicity, let's say species 0 interacts strongly with resource 1,\n",
        "    # and species 1 interacts strongly with resource 2, etc.\n",
        "    for i in range(num_species):\n",
        "        species_mask = (data['taxonomic_id'] == i)\n",
        "        # Link species to a specific resource preference for interaction\n",
        "        resource_idx = i % num_resources\n",
        "        data['communication_signal'][species_mask] += data[f'resource_pref_{resource_idx+1}'][species_mask] * 2\n",
        "\n",
        "    # Add patterns based on gene presence (e.g., specific gene enhances communication)\n",
        "    # Example: If gene_1 is present, it boosts communication signal\n",
        "    data['communication_signal'] += data['gene_1'] * 3 # Gene_1 boosts signal by 3 if present\n",
        "    data['communication_signal'] += data['gene_5'] * data['gene_10'] * 2 # Synergistic effect between gene_5 and gene_10\n",
        "\n",
        "    # Ensure communication signal remains positive\n",
        "    data['communication_signal'] = np.maximum(0, data['communication_signal'])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    print(\"Synthetic data generation complete. First 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataFrame Info:\")\n",
        "    df.info()\n",
        "    return df\n",
        "\n",
        "# Call the function to generate a dataset with at least 5,000,000 entries\n",
        "num_entries = 5_000_000 # 5 million entries\n",
        "synthetic_data_df = generate_synthetic_data(num_entries)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic data generation complete. First 5 rows:\n",
            "   sample_id  gene_1  gene_2  gene_3  gene_4  gene_5  gene_6  gene_7  gene_8  \\\n",
            "0          0       0       1       1       0       0       1       1       0   \n",
            "1          1       1       0       1       1       0       1       0       0   \n",
            "2          2       0       1       0       0       1       0       1       0   \n",
            "3          3       1       1       0       1       1       1       1       1   \n",
            "4          4       0       0       1       1       1       1       0       0   \n",
            "\n",
            "   gene_9  ...  gene_13  gene_14  gene_15  taxonomic_id  \\\n",
            "0       1  ...        1        1        0             2   \n",
            "1       1  ...        0        0        0             7   \n",
            "2       0  ...        0        1        0             3   \n",
            "3       0  ...        1        1        1             0   \n",
            "4       0  ...        0        1        0             3   \n",
            "\n",
            "   growth_rate_potential  resource_pref_1  resource_pref_2  resource_pref_3  \\\n",
            "0               6.891323         0.447908         0.599731         0.636555   \n",
            "1               0.194224         0.080418         0.569187         0.952193   \n",
            "2               7.462168         0.363317         0.309951         0.743602   \n",
            "3               3.189792         0.562021         0.747698         0.092728   \n",
            "4               8.018450         0.737476         0.965546         0.116475   \n",
            "\n",
            "   resource_pref_4  communication_signal  \n",
            "0         0.427783              1.950325  \n",
            "1         0.674490              5.848942  \n",
            "2         0.222799              4.906666  \n",
            "3         0.946857              8.567212  \n",
            "4         0.895146              6.522087  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000000 entries, 0 to 4999999\n",
            "Data columns (total 23 columns):\n",
            " #   Column                 Dtype  \n",
            "---  ------                 -----  \n",
            " 0   sample_id              int64  \n",
            " 1   gene_1                 int64  \n",
            " 2   gene_2                 int64  \n",
            " 3   gene_3                 int64  \n",
            " 4   gene_4                 int64  \n",
            " 5   gene_5                 int64  \n",
            " 6   gene_6                 int64  \n",
            " 7   gene_7                 int64  \n",
            " 8   gene_8                 int64  \n",
            " 9   gene_9                 int64  \n",
            " 10  gene_10                int64  \n",
            " 11  gene_11                int64  \n",
            " 12  gene_12                int64  \n",
            " 13  gene_13                int64  \n",
            " 14  gene_14                int64  \n",
            " 15  gene_15                int64  \n",
            " 16  taxonomic_id           int64  \n",
            " 17  growth_rate_potential  float64\n",
            " 18  resource_pref_1        float64\n",
            " 19  resource_pref_2        float64\n",
            " 20  resource_pref_3        float64\n",
            " 21  resource_pref_4        float64\n",
            " 22  communication_signal   float64\n",
            "dtypes: float64(6), int64(17)\n",
            "memory usage: 877.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e2e5bea"
      },
      "source": [
        "## Implement Self-Supervised Learning Model\n",
        "\n",
        "### Subtask:\n",
        "Design and implement a self-supervised learning model architecture, such as a neural network, and prepare the synthetic dataset for training. The model will be designed to predict the 'communication_signal' as its self-supervised pre-training task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15b02fa1"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to separate the features and the target variable from the `synthetic_data_df` and convert them into PyTorch tensors, as required by the subtask instructions. This prepares the data for use with PyTorch's Dataset and DataLoader.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "258de8b9",
        "outputId": "f743f739-b115-44de-daa9-c11a4cdd10ac"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 1. Separate features and target\n",
        "# All columns except 'sample_id' and 'communication_signal' are features\n",
        "feature_cols = [col for col in synthetic_data_df.columns if col not in ['sample_id', 'communication_signal']]\n",
        "X = synthetic_data_df[feature_cols].values\n",
        "y = synthetic_data_df['communication_signal'].values\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1) # unsqueeze for single output feature\n",
        "\n",
        "print(f\"Features tensor shape: {X_tensor.shape}\")\n",
        "print(f\"Target tensor shape: {y_tensor.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features tensor shape: torch.Size([5000000, 21])\n",
            "Target tensor shape: torch.Size([5000000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13300739"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a custom PyTorch Dataset class to handle the feature and target tensors, and then instantiate a DataLoader to enable efficient batch processing during training, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4132a7f",
        "outputId": "9fb73491-737b-4642-d9a1-e33074e5c496"
      },
      "source": [
        "class MetagenomicDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "# Create an instance of the custom Dataset\n",
        "metagenomic_dataset = MetagenomicDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Instantiate a DataLoader\n",
        "batch_size = 64  # A common batch size for training\n",
        "metagenomic_dataloader = DataLoader(metagenomic_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"Number of samples in Dataset: {len(metagenomic_dataset)}\")\n",
        "print(f\"Number of batches in DataLoader: {len(metagenomic_dataloader)}\")\n",
        "\n",
        "# Verify a batch\n",
        "for features, targets in metagenomic_dataloader:\n",
        "    print(f\"First batch features shape: {features.shape}\")\n",
        "    print(f\"First batch targets shape: {targets.shape}\")\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in Dataset: 5000000\n",
            "Number of batches in DataLoader: 78125\n",
            "First batch features shape: torch.Size([64, 21])\n",
            "First batch targets shape: torch.Size([64, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62c90ca5"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is prepared, I will define a neural network model using `torch.nn.Module`, specifying its architecture with linear layers and activation functions. I will also define the Mean Squared Error as the loss function and Adam as the optimizer, as required by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1afaaec5",
        "outputId": "91830c1c-0a72-4154-f5ed-7e429dbaed0a"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 5. Define a neural network model\n",
        "class CommunicationPredictor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CommunicationPredictor, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(64, 32)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(32, 1) # Output a single 'communication_signal'\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.layer1(x))\n",
        "        x = self.relu2(self.layer2(x))\n",
        "        x = self.relu3(self.layer3(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Get input dimension from our features tensor\n",
        "input_dimension = X_tensor.shape[1]\n",
        "\n",
        "# Instantiate the model\n",
        "model = CommunicationPredictor(input_dimension)\n",
        "print(f\"Model architecture:\\n{model}\")\n",
        "\n",
        "# 6. Define the Mean Squared Error (MSE) as the loss function\n",
        "criterion = nn.MSELoss()\n",
        "print(f\"Loss function: {criterion}\")\n",
        "\n",
        "# 7. Choose an optimizer (e.g., Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "print(f\"Optimizer: {optimizer}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture:\n",
            "CommunicationPredictor(\n",
            "  (layer1): Linear(in_features=21, out_features=128, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (layer3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "Loss function: MSELoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3042b8ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the model, loss function, and optimizer are defined, I will implement a training loop to train the neural network model using the `metagenomic_dataloader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b0c82a",
        "outputId": "89ac34a0-4fc2-4172-be96-7fbf1df32fe2"
      },
      "source": [
        "num_epochs = 5\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for batch_features, batch_targets in metagenomic_dataloader:\n",
        "        # Move data to the device\n",
        "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_features)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(metagenomic_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting model training...\n",
            "Epoch [1/5], Loss: 2.2619\n",
            "Epoch [2/5], Loss: 2.1327\n",
            "Epoch [3/5], Loss: 2.1197\n",
            "Epoch [4/5], Loss: 2.1144\n",
            "Epoch [5/5], Loss: 2.1108\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca3a5133"
      },
      "source": [
        "## Train and Fine-tune Model for Communication Decoding\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the pre-trained model's performance in decoding microbial communication by predicting the 'communication_signal' and calculating a regression metric, aiming for over 85% accuracy (interpreted as R-squared).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b663c498"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to evaluate the trained model's performance on the synthetic dataset by predicting the 'communication_signal' and calculating the R-squared metric to assess accuracy, following the detailed instructions provided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa885251",
        "outputId": "6497903a-c545-4d54-cced-7d59123382a0"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create empty lists to store all predicted values and true target values\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "print(\"Evaluating model performance...\")\n",
        "# 2. Disable gradient calculations for inference\n",
        "with torch.no_grad():\n",
        "    # 4. Iterate through the metagenomic_dataloader\n",
        "    for batch_features, batch_targets in metagenomic_dataloader:\n",
        "        # 5. Move features and targets to the device\n",
        "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "        # 6. Get predictions from the model\n",
        "        outputs = model(batch_features)\n",
        "\n",
        "        # 7. Append the predictions and true targets (converted to NumPy arrays) to their respective lists\n",
        "        all_predictions.append(outputs.cpu().numpy())\n",
        "        all_targets.append(batch_targets.cpu().numpy())\n",
        "\n",
        "# 8. Concatenate all predictions and true targets to get a single array for each\n",
        "predictions_np = np.concatenate(all_predictions, axis=0)\n",
        "targets_np = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "# 10. Calculate the R-squared score\n",
        "r_squared = r2_score(targets_np, predictions_np)\n",
        "\n",
        "# 11. Print the calculated R-squared score and indicate whether it meets the target of 85% (0.85)\n",
        "print(f\"R-squared score on the synthetic dataset: {r_squared:.4f}\")\n",
        "\n",
        "if r_squared > 0.85:\n",
        "    print(\"Model performance meets the target of over 85% R-squared.\")\n",
        "else:\n",
        "    print(\"Model performance does NOT meet the target of over 85% R-squared.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model performance...\n",
            "R-squared score on the synthetic dataset: 0.6100\n",
            "Model performance does NOT meet the target of over 85% R-squared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcc8395"
      },
      "source": [
        "## Refine Model Training for Improved Communication Decoding\n",
        "\n",
        "### Subtask:\n",
        "The model's R-squared score (0.6100) did not meet the target of over 85%. To improve performance, refine the model training by adjusting hyperparameters such as the number of epochs and learning rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b52ed9"
      },
      "source": [
        "**Reasoning**:\n",
        "The model's R-squared score did not meet the target. To improve performance, I will increase the number of epochs for training to allow the model more time to learn the patterns in the data, as per the instructions. I will keep the learning rate unchanged for now and focus on the impact of increased epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6f8a3a2",
        "outputId": "f430a747-68e3-48ea-a76d-0e725526a959"
      },
      "source": [
        "num_epochs = 20 # Increased number of epochs\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Starting model training with increased epochs...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for batch_features, batch_targets in metagenomic_dataloader:\n",
        "        # Move data to the device\n",
        "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_features)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(metagenomic_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting model training with increased epochs...\n",
            "Epoch [1/20], Loss: 2.1076\n",
            "Epoch [2/20], Loss: 2.1056\n",
            "Epoch [3/20], Loss: 2.1043\n"
          ]
        }
      ]
    }
  ]
}